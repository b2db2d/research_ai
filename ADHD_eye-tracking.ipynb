https://colab.research.google.com/drive/1aptph1M87-Jm0t0SZNpdowjSV3JWbfPR?usp=sharing

# Import necessary libraries
import numpy as np
import pandas as pd
import h5py  # Import h5py to read HDF5 files
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the HDF5 MATLAB file
file_path = '/content/drive/MyDrive/Pupil_dataset.mat'

# Open the HDF5 file
with h5py.File(file_path, 'r') as f:
    # Access the structure array
    pupil_data = f['Pupil_data']
    print("Structure keys:", list(pupil_data.keys()))

    # Helper function to extract data from HDF5 references
    def extract_data_from_reference(ref, dataset):
        """Extracts data from HDF5 object references."""
        data = []
        for item in ref:
            obj_ref = item[0]
            # Dereference to get the actual dataset
            deref_data = dataset[obj_ref]
            # Extract the data
            extracted = deref_data[()]
            data.append(extracted.item() if extracted.size == 1 else extracted.flatten())
        return data

    # Extracting fields from the structure using references
    subjects = extract_data_from_reference(pupil_data['Subject'], f)
    ages = extract_data_from_reference(pupil_data['Age'], f)
    groups = extract_data_from_reference(pupil_data['Group'], f)
    task_data = extract_data_from_reference(pupil_data['Task_data'], f)
    task_epochs = extract_data_from_reference(pupil_data['Task_epocs'], f)
    wisc = extract_data_from_reference(pupil_data['Wisc'], f)

    # Convert the data into a DataFrame
    df = pd.DataFrame({
        'Subject': subjects,
        'Age': ages,
        'Group': groups,
        'Task_data': task_data,
        'Task_epochs': task_epochs,
        'WISC': wisc
    })

# Display the first few rows of the DataFrame
print("Original DataFrame:")
print(df.head())

# Extract a single value from lists in each column (e.g., use the first value or average)
df['Group'] = df['Group'].apply(lambda x: x[0] if isinstance(x, np.ndarray) else x)
df['Task_data'] = df['Task_data'].apply(lambda x: x[1] if isinstance(x, np.ndarray) else x) # Example: use second element
df['Task_epochs'] = df['Task_epochs'].apply(lambda x: x[4] if isinstance(x, np.ndarray) else x) # Example: use fifth element
df['WISC'] = df['WISC'].apply(lambda x: x[4] if isinstance(x, np.ndarray) else x) # Example: use fifth element

# Display the processed DataFrame
print("\nProcessed DataFrame:")
print(df.head())

# Check for missing values and preprocess data
df = df.dropna()  # Remove missing values

# Convert categorical labels to numeric if necessary
df['Group'] = pd.factorize(df['Group'])[0]

# Extract features and labels
X = df[['Age', 'Task_data', 'Task_epochs', 'WISC']]  # Features
y = df['Group']  # Labels

# Check if the dataset is large enough
if len(df) < 2:
    print("Dataset too small for splitting. Using entire dataset for demonstration.")
    X_train, y_train = X, y
    X_test, y_test = X, y
else:
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check class distribution to adjust cv parameter
unique, counts = np.unique(y_train, return_counts=True)
class_distribution = dict(zip(unique, counts))
print(f"Class distribution in training set: {class_distribution}")

# Define the SVM model with RBF kernel
svm_model = SVC(kernel='rbf')

# Adjust cv parameter based on class distribution
cv_folds = min(min(counts), 2)  # Use 2 folds for very small datasets

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'C': [26.5, 26.75, 27, 27.25, 27.5, 27.75, 28],
    'gamma': [2**-14.5, 2**-14.75, 2**-15, 2**-15.25, 2**-15.5, 2**-15.75, 2**-16]
}

# Perform grid search for hyperparameter tuning
grid_search = GridSearchCV(svm_model, param_grid, scoring='accuracy', cv=cv_folds)
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print(f"Best parameters: {best_params}")

# Train the SVM model with the best parameters
svm_model = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'])
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)
f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Display classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, zero_division=1))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Visualize decision boundaries (only possible in 2D)
plt.figure(figsize=(10, 6))

# For visualization, we can only use two features
X_vis = X_train[['Age', 'WISC']].values
y_vis = y_train

# Fit SVM for visualization
svm_vis = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma']).fit(X_vis, y_vis)

# Create a mesh to plot the decision boundaries
h = .02
x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1
y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Plot decision boundaries
Z = svm_vis.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

# Plot the training points
scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, cmap=plt.cm.coolwarm, edgecolors='k')
plt.title('SVM Decision Boundaries with RBF Kernel')
plt.xlabel('Age')
plt.ylabel('WISC Score')
plt.legend(*scatter.legend_elements(), title="Classes")
plt.show()
